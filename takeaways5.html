<!DOCTYPE html>
<html lang="en">

<head>
<meta charset="utf-8" />
<link rel="stylesheet" href="https://dokie.li/media/css/lncs.css" />
<title>Takeaways for Week 5</title>
</head>

<body>

<h1>Takeaways for Week 5</h1>

<h2>Topics</h2>

<h3>Knowledge Representation on the Web</h3>

<p class="counter">
By comparing a knowledge base used in the web with a knowledge base used in e.g. companies, we can identify i) openness, ii) decentralization and iii) the location independence as the main differences and also as the main accompanied problems. To achieve a knowledge organization across the web it is crucial to address these problems by structuring its contents and implementing a semantic representation. Several semantic web languages are based on RDF which tries with minimal semantics and restrictions to express knowledge in a decentralized setting such that any web-entity is abstracted as 'resource'. However, the actual relationship among these resources is not addressed by RDF. OWL instead is capable of identifying logically and theoretically sound reasonings between resources. The trade-off between formally sound definitions (i.e. with OWL) and the convenience and simplicity (i.e. with RDF) is also a temporal question. A comprehensive formal abstraction of the entire web can only be achieved by long-term solutions which are mostly uninteresting for business companies (e.g. Google) since they focus on short-term solutions. These global companies, however, are important to push the formal abstraction forward and create an actual semantic-web.
</p>

<h3>Knowledge Modeling</h3>

<p class="counter">
The knowledge of an expert is useless and can not be applied if it exists only in the expert's head. There must be a way to extract this knowledge and translate it into an effective and computable form. This gap between an expert and an actually usable knowledge base is known as 'knowledge acquisition bottleneck'. It describes the difficulties of translating the knowledge present in someone's head (the transmitter) via, i.e. natural language and of converting this information into a processable form, i.e. a database structure, by someone else (the receiver). However, there is no actual concept that can be applied to close this gap. Knowledge extraction is a process that needs to be modeled and designed by keeping a particular context in mind. Modeling knowledge is about making choices and is the actual result by addressing all individual needs and circumstances. The Functional Requirements for Bibliographic Records (FRBR) is one example that represents one particular model to define an abstraction to organize different kinds of literature and related works to create an abstraction for different individuals. Due to this creative and context related process, Clancey (1990) defined the bottleneck metaphor is misleading.
</p>

<h2>Literature</h2>

<h3 data-ranking="neutral">Bizer et al. 2009</h3>

<p class="counter">
The study from Bizer et al. (2009) introduces DBpedia which can be described as an interface to close the gap between knowledge experts and the semantic representation of their information. Wikipedia serves as a kind of 'text editor' to create and represent the knowledge in a natural language; and  DBpedia organizes this knowledge in an automatic process to a computable representation. To achieve this, RDF is used as the main language to represent information in a semantic context. The several different languages in which one article on Wikipedia can exist, are linked to the English base article. This offers the possibility to uncover incomplete articles across different languages. During the implementation, it was detected that the 'infoboxes' from Wikipedia do not follow one specific convention. Authors can use these boxes in many different ways, which makes the mining process for DBpedia rather complex. This uncovers also the actual problem of the web itself. There is no standard for the non-expert domain which forces the end-users (i.e. Wikipedia authors) to use one particular way for knowledge representation. This main issue makes the importance and existence of such platforms like DBpedia necessary in the first place.
</p>

<h3>Vrandecic and Krotzsch 2014</h3>

<p class="counter">
Wikidata, which was also launched by the Wikimedia Foundation who initiated Wikipedia, is a knowledge base to organizes information from Wikipedia, and especially beyond Wikipedia, at one centralized place in a structured manner [Vrandecic and Krotzsch 2014]. The information at Wikidata is a collaborative result which offers everyone the possibility to edit or add knowledge. Instead of having dedicated landing-pages for each language, like Wikipedia is organized, Wikidata is using a centralized approach. Every entity consists only once, but in different languages.  Through this centralization, each different resource can be expressed by a unique identifier. These identifier servers as URI and makes the resource accessible as well as processible by third-party applications. All entities at Wikidata are semantically expressed via the RDF language which allows complex queries across the data. The centralization, the semantic representation, and the extensibility of one entity through external resources and links, i.e. plurality, contribute all together towards 'The Web of Data'.
</p>

<h3>Beek et al. 2016</h3>

<p class="counter">
According to Beek et al. (2016), the semantic web with all its advantages is rather a utopia than reality in its current state of implementation. The authors introduced the Linked Open Data (LOD) laundromat and addresses, among others, one main problem of the semantic web—data cleaning. Still, current websites are implemented for humans and the presentation layer is still the main concern for the developer since this is the connection to the end-user. Therefore, if the application is not explicitly made for LOD, e.g. Wikidata, most websites do not provide a semantic representation of their data. However, existing semantic representations are still made by humans and contain errors or are isolated in their ecosystem. Instead of cleaning each data on a case-by-case basis, the LOD laundromat automatically collects LOD from the web, cleans broken and misused LOD representations, links data-sets together, and publishes the data at one single place. By providing an API, developers or agents have access to the centralized data and can traverse the entire scraped linked data-set.
</p>

<h3>Schreiber et al. 2008</h3>

<p class="counter">
Replace this text with your takeaway of the paper by Schreiber et al. Cite it as [Schreiber et al. 2008. Use between 100 and 200 words.
</p>

<h2>References</h2>

<ul>
<li>Bizer et al. 2009. <a href="http://dx.doi.org/10.1016/j.websem.2009.07.002">DBpedia - A crystallization point for the Web of Data.</a> Web Semantics, 7(3).</li>
<li>Vrandecic and Krotzsch. 2014. <a href="http://dx.doi.org/10.1145/2629489">Wikidata: a free collaborative knowledgebase.</a> Communications of the ACM 57(10).</li>
<li>Beek et al. 2016. <a href="http://dx.doi.org/10.1109/mic.2016.43">LOD Laundromat: Why the Semantic Web Needs Centralization (Even If We Don’t Like It).</a> IEEE Internet Computing, 20(2).</li>
<li>Schreiber et al. 2008. <a href="http://dx.doi.org/10.1016/j.websem.2008.08.001">Semantic annotation and search of cultural-heritage collections: The MultimediaN E-Culture demonstrator.</a> Web Semantics, 6(4).</li>
<li>Clancey. 1990. <a href="http://dx.doi.org/10.1007/978-1-4613-1531-5_5">The Knowledge Level Reinterpreted: Modeling How Systems Interact.</a> Machine Learning, 92. </li>
</ul>




<script src="https://raw.githack.com/ucds-vu/ko2020-portfolio-template/master/scripts.js"></script>

</body>
</html>

